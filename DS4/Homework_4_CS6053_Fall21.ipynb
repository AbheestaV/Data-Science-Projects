{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Homework_4_CS6053_Fall21.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfWyuZ_IaO-P"
      },
      "source": [
        "# Foundation of Data Science\n",
        "## Homework 4 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22PYkUOlaO-T"
      },
      "source": [
        "Student Name: \n",
        "\n",
        "Student Netid:\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISZWB6vTaO-T"
      },
      "source": [
        "### Part 1: Critique this plan (10 points)\n",
        "1\\. After a few beers your CIO invited his buddy from Green Berry consulting to propose a project using data mining to improve the targeting of the new service that you have been a principal in developing. The service has been quite successful so far, being marketed over the last 6 months via your ingenious, and very inexpensive, word-of-mouth campaign. You've already garnered a pretty large customer base without any targeting, and you've been seeing this success as your best stepping stone to bigger and better things in the firm. \n",
        "\n",
        "After some reflection, you've decided that your best course of action is to play a key role in ensuring the success of this data mining project as well. You agree with your CIO's statement in a meeting with Green Berry, that accurate targeting might cost-effectively expand your audience substantially to consumers that word-of-mouth would not reach. You accept that what Green Berry says about the characteristics of your service is accurate.\n",
        "\n",
        "Based on what we have covered in class, identify the four most serious weaknesses/flaws in this abridged version of Green Berry's proposal, and suggest how to ameliorate them.  Your answer should be 4 bullet points, each comprising 2-4 sentences: 1-2 sentences stating each weakness, and 1-2 sentences suggesting a better alternative.  Maximal credit will be given when the 4 points are as independent as possible.\n",
        "\n",
        "\n",
        "```\n",
        "--- -------------------------------------------------------------------------- ---\n",
        "                            Targeted Audience Expansion             \n",
        "                      Prepared by Green Berry Consulting, Inc.\n",
        "\n",
        "Your problem is to expand the audience of your new service.  We (Green Berry) have a \n",
        "large database of consumers who can be targeted.  We will build a predictive model \n",
        "to estimate which of these consumers are the most likely to adopt the product, and\n",
        "then target them with the special offer you have designed.\n",
        "\n",
        "More specifically, we will build a decision tree (DT) model to predict adop-\n",
        "tion of the service by a consumer, based on the data on your current customers of \n",
        "this service.  The model will be based on their demographics and their usage of \n",
        "the service. We believe that DT is the best choice of method be-\n",
        "cause it is a tried-and-true modeling technique, and we can easily \n",
        "interpret the model to infer whether the attributes make sense. We will apply the model to our large \n",
        "database of consumers, and select out those who have not yet subscribed and whom\n",
        "the DT model predicts to be likely to subscribe.  To these we will send \n",
        "the targeted offer. As this is a fixed-profit-per-customer service, this also \n",
        "will in effect rank them by expected profit.\n",
        "--- -------------------------------------------------------------------------- ---\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EsboGuO-Q1u"
      },
      "source": [
        "**Write your answer here.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SnyptqvaO-c"
      },
      "source": [
        "### Part 2: Working with Time Series (5 points)\n",
        "\n",
        "Here we will analyze a timeseries of number of Yellow Fever cases from a specific Asian country by year. The data is from 1700 â€“ 2008 (use file cases.csv for this section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABtgguSaaO-c"
      },
      "source": [
        "1\\. Load the timeseries data set, and prepare the dataset by converting the variables to date-time format (hint: use date tools and the library statsmodels). (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-jil5EzaO-c"
      },
      "source": [
        "#write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2NaDCfD78vV"
      },
      "source": [
        "2\\. Plot the autocorrelation function (ACF) for the cases timeseries (hint: use statsmodels plot_acf for that). (2 points)\n",
        "\n",
        "To learn more about how to interpret these graphs, you may find this useful: https://medium.com/analytics-vidhya/interpreting-acf-or-auto-correlation-plot-d12e9051cd14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voyyfG2f-gkJ"
      },
      "source": [
        "#write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hZH0NvHJaO-d"
      },
      "source": [
        "3\\. An approach to assess the presence of a significant autocorrelation in the data is to use the Durbin-Waton (DW) statistic. The value of the DW statistic is close to 2 if the errors are uncorrelated. What is the DW for our data? Does this suggest that the data has a relatively high or a relatively low autocorrelation? (2 point) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWMRwxXtaO-d"
      },
      "source": [
        "#write your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM-L4qDd-VRN"
      },
      "source": [
        "### Part 3: Comparison of Models (10 points)\n",
        "\n",
        "In this part we will be looking at data generated by particle physicists to test if machine learning can help classify whether certain particle decay experiments identify the presence of a Higgs Boson. One does not need to know anything about particle physics to do well here, but if you are curious, full feature and data descriptions can be found here:\n",
        "\n",
        "- https://www.kaggle.com/c/higgs-boson/data\n",
        "- http://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf\n",
        "\n",
        "The goal of this assignment is to practice how to compare different classification models using the area under the ROC curve (AUC). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJZxtmY-tvA"
      },
      "source": [
        "1\\. Create a data preparation and cleaning function that does the following:\n",
        "- Has a single input that is a file name string\n",
        "- Reads data (the data is comma separated, has a row header and the first column `EventID` is the index) into a pandas `dataframe`\n",
        "- Cleans the data\n",
        "  - Convert the feature `Label` to numeric (choose the minority class to be equal to 1)\n",
        "    - Create a feature `Y` with numeric label\n",
        "    - Drop the feature `Label`\n",
        "  - If a feature has missing values (i.e., `-999`): \n",
        "    - Create a dummy variable for the missing value\n",
        "      - Call the variable `orig_var_name` + `_mv` where `orig_var_name` is the name of the actual var with a missing value\n",
        "      - Give this new variable a 1 if the original variable is missing\n",
        "    - Replace the missing value with the average of the feature (make sure to compute the mean on records where the value isn't missing). You may find pandas' `.replace()` function useful.\n",
        "- After the above is done, rescales the data so that each feature has zero mean and unit variance (hint: look up sklearn.preprocessing)\n",
        "- Returns the cleaned and rescaled dataset\n",
        "\n",
        "Hint: as a guide, this function can easily be done in less than 15 lines. (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXR97pcc-a4p"
      },
      "source": [
        "#write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAnaZJ4V-6Sw"
      },
      "source": [
        "2\\. Clean the two data files included in this assignment (`boson_training_cut_2000.csv` and `boson_testing_cut.csv`) and use them as training and testing data sets.\n",
        "\n",
        "(1 Point)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJaEeYEX_FtL"
      },
      "source": [
        "#write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjoTuYaO_LvT"
      },
      "source": [
        "3\\. On the training set, build the following models:\n",
        "\n",
        "- A KNN classifier using sklearn's `neighbors.KNeighborsClassifier`. For this model, use the standard parameters. \n",
        "- An sklearn classifier of your choice (e.g., `tree.DecisionTreeClassifierSVM`, `svm.svc()`, `linear_model.LogisticRegression()`, `naive_bayes.GaussianNB` etc.).\n",
        "\n",
        "For each model above, plot the ROC curve of both models on the same plot. Make sure to use the test set for computing and plotting. In the legend, also print out the Area Under the ROC (AUC) for reference.\n",
        "\n",
        "(Hint: to get the prediction thresholds that are necessary for the AUC, use function predict_proba() for KNN and for the classifier you choose if it has it. If you work with SVM, use function decision_function().)\n",
        "\n",
        "(4 Points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCXH1c69SiL_"
      },
      "source": [
        "#write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQqzhaJjSjWQ"
      },
      "source": [
        "4\\. Which of the two models is generally better at ranking the test set? Are there any classification thresholds where the model identified above as \"better\" would underperform the other in any classification metric (such as recall)? (3 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCG1kTeNV8qg"
      },
      "source": [
        "**Write your analysis here.**"
      ]
    }
  ]
}